name: slurm-single-node

on:
  push:
  pull_request:

jobs:
  slurm:
    runs-on: ubuntu-24.04

    steps:
      - uses: actions/checkout@v4

      - name: Install Slurm + Munge (+ SlurmDBD + MariaDB)
        run: |
          set -euxo pipefail
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends \
            munge \
            slurmctld slurmd slurm-client slurmdbd \
            mariadb-server

      - name: Configure Munge
        run: |
          set -euxo pipefail

          # Create a fresh key
          sudo dd if=/dev/urandom of=/etc/munge/munge.key bs=1 count=1024 status=none
          sudo chown munge:munge /etc/munge/munge.key
          sudo chmod 0400 /etc/munge/munge.key

          # Start munge w new key
          sudo systemctl enable munge
          sudo systemctl restart munge
          sudo systemctl status --no-pager munge

      - name: Configure single-node Slurm (+ slurmdbd)
        run: |
          set -euxo pipefail

          HOST="$(hostname -s)"

          # Ensure HOST resolves locally!
          if ! getent hosts "$HOST" >/dev/null 2>&1; then
            echo "127.0.1.1 $HOST" | sudo tee -a /etc/hosts >/dev/null
          fi

          # Create slurm service user (if not present)
          if ! id -u slurm >/dev/null 2>&1; then
            sudo useradd -r -m -U -d /var/lib/slurm -s /usr/sbin/nologin slurm
          fi

          CPUS="$(nproc)"
          MEM_MB="$(awk '/MemTotal/ {print int($2/1024)}' /proc/meminfo)"

          sudo install -d -o slurm -g slurm /var/lib/slurm/slurmctld
          sudo install -d -o slurm -g slurm /var/lib/slurm/slurmd
          sudo install -d -o slurm -g slurm /var/log/slurm
          sudo install -d -o root  -g root  /etc/slurm

          # --- MariaDB bootstrap for Slurm accounting (minimal) ---
          sudo systemctl enable --now mariadb
          # Create DB + user SlurmDBD will use
          sudo mysql -e "CREATE DATABASE IF NOT EXISTS slurm_acct_db;"
          sudo mysql -e "CREATE USER IF NOT EXISTS 'slurm'@'localhost' IDENTIFIED BY 'slurm';"
          sudo mysql -e "GRANT ALL PRIVILEGES ON slurm_acct_db.* TO 'slurm'@'localhost';"
          sudo mysql -e "FLUSH PRIVILEGES;"

          # --- slurm.conf ---
          sudo tee /etc/slurm/slurm.conf >/dev/null <<EOF
          ClusterName=ci
          SlurmctldHost=${HOST}
          SlurmUser=slurm

          AuthType=auth/munge
          StateSaveLocation=/var/lib/slurm/slurmctld
          SlurmdSpoolDir=/var/lib/slurm/slurmd

          SlurmctldPort=6817
          SlurmdPort=6818

          # Scheduling/allocation
          SelectType=select/cons_tres
          SelectTypeParameters=CR_Core

          # No cgroup enforcement
          ProctrackType=proctrack/linuxproc
          TaskPlugin=task/none

          # Avoid fatal "Configured MailProg is invalid" on minimal images
          MailProg=/bin/true

          # Accounting via SlurmDBD
          AccountingStorageType=accounting_storage/slurmdbd

          # Logging
          SlurmctldLogFile=/var/log/slurm/slurmctld.log
          SlurmdLogFile=/var/log/slurm/slurmd.log

          # Single-node definition
          NodeName=${HOST} CPUs=${CPUS} RealMemory=${MEM_MB} State=UNKNOWN
          PartitionName=debug Nodes=${HOST} Default=YES MaxTime=INFINITE State=UP
          EOF

          # --- slurmdbd.conf (permissions MUST be strict) ---
          sudo tee /etc/slurm/slurmdbd.conf >/dev/null <<'EOF'
          AuthType=auth/munge
          DbdHost=localhost
          SlurmUser=slurm

          LogFile=/var/log/slurm/slurmdbd.log
          PidFile=/run/slurmdbd.pid

          StorageType=accounting_storage/mysql
          StorageHost=localhost
          StoragePort=3306
          StorageUser=slurm
          StoragePass=slurm
          StorageLoc=slurm_acct_db
          EOF
          sudo chown slurm:slurm /etc/slurm/slurmdbd.conf
          sudo chmod 0600 /etc/slurm/slurmdbd.conf

          # /etc/slurm-llnl compatibility
          if [ -d /etc/slurm-llnl ] && [ ! -L /etc/slurm-llnl ]; then
            sudo rm -rf /etc/slurm-llnl
          fi
          if [ ! -e /etc/slurm-llnl ]; then
            sudo ln -s /etc/slurm /etc/slurm-llnl
          fi

          sudo chown -R slurm:slurm /var/lib/slurm /var/log/slurm

          echo "=== /etc/slurm/slurm.conf ==="
          sed -n '1,220p' /etc/slurm/slurm.conf
          echo "=== /etc/slurm/slurmdbd.conf (exists) ==="
          sudo ls -l /etc/slurm/slurmdbd.conf

      - name: Start Slurm daemons (mariadb -> slurmdbd -> slurmctld -> slurmd)
        run: |
          set -euxo pipefail

          # Validate slurmctld config (supported)
          sudo slurmctld -t

          # "Validate" slurmdbd config: start in foreground briefly.
          # If config is good, it will keep running and timeout returns 124.
          # If config is bad, slurmdbd exits quickly with a non-124 error.
          set +e
          timeout 2s sudo slurmdbd -D -s -v
          rc=$?
          set -e
          if [ "$rc" -ne 124 ] && [ "$rc" -ne 0 ]; then
            echo "slurmdbd failed to start (rc=$rc) â€” config likely invalid"
            exit "$rc"
          fi

          sudo systemctl enable --now mariadb
          sudo systemctl enable --now slurmdbd
          sudo systemctl enable --now slurmctld
          sudo systemctl enable --now slurmd

          sudo systemctl status --no-pager mariadb
          sudo systemctl status --no-pager slurmdbd
          sudo systemctl status --no-pager slurmctld
          sudo systemctl status --no-pager slurmd


      - name: Bootstrap accounting (cluster/account/user)
        run: |
          set -euxo pipefail

          # Create cluster to match ClusterName=ci (idempotent with || true)
          sudo sacctmgr -i add cluster ci || true

          # Create an account + associate the GitHub Actions user
          sudo sacctmgr -i add account default Description="CI default" Organization="ci" || true
          sudo sacctmgr -i add user runner Account=default || true

          echo "=== Associations ==="
          sudo sacctmgr -n show assoc format=Cluster,Account,User || true

      - name: Smoke test (sinfo / scontrol / sbatch)
        run: |
          set -euxo pipefail

          echo "=== Initial cluster state ==="
          sinfo -Nel || true
          scontrol show node || true

          HOST="$(hostname -s)"

          echo "Waiting for node '$HOST' to become ready..."
          for i in $(seq 1 60); do
            state="$(sinfo -h -N -n "$HOST" -o "%T" 2>/dev/null || true)"
            echo "[$i/60] state='${state}'"
            case "$state" in
              idle|mix|alloc|completing)
                echo "Node is ready: $state"
                break
                ;;
            esac
            sleep 1
          done

          echo "=== Post-wait cluster state ==="
          sinfo -Nel
          scontrol show node

          cat > job.sh <<'EOF'
          #!/usr/bin/env bash
          set -euxo pipefail
          echo "Hello from Slurm on $(hostname)"
          EOF
          chmod +x job.sh

          jid="$(sbatch --parsable -p debug -A default -J ci-test --wrap "./job.sh")"
          echo "Submitted: $jid"
          squeue -j "$jid" || true

          for i in $(seq 1 60); do
            if ! squeue -j "$jid" >/dev/null 2>&1; then
              break
            fi
            squeue -j "$jid" || true
            sleep 1
          done

